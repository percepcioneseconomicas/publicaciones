{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del Sentimiento de las noticias económicas en Chile 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score"
   ]
  },
  {
   "source": [
    "En primer lugar se cargan los datos "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   response                                              texto\n",
       "0       1.0  En cuanto a las colocaciones, estas alcanzaron...\n",
       "1       0.0  En el año 2009 el mundo enfrentó una crisis fi...\n",
       "2       1.0  Es para mí motivo de gran satisfacción compart...\n",
       "3       1.0  Si tuviera que resumir la gestión 2012 de Cruz...\n",
       "4       1.0  El crecimiento anteriormente mencionado redund..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>texto</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>En cuanto a las colocaciones, estas alcanzaron...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>En el año 2009 el mundo enfrentó una crisis fi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>Es para mí motivo de gran satisfacción compart...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>Si tuviera que resumir la gestión 2012 de Cruz...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>El crecimiento anteriormente mencionado redund...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/percepcioneseconomicas/publicaciones/main/sentiment_data/sentiment_data.csv')\n",
    "data['response'][data['response'] == 2.0] = 0\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data = data[['response', 'texto']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Dimensiones de los datos: \\n', data.shape, '\\n')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows en cada categoría de la variable y\n",
    "data['response'].value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Número de palabras\n",
    "x = []\n",
    "[x.append(len(e.split())) for e in data['texto']]\n",
    "print('Número de palabras de la noticia más larga: \\n',  max(x), '\\n')\n",
    "print('Número de palabras de la noticia más corta: \\n',  min(x), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Noticia aleatoria\n",
    "k = np.random.randint(0, len(data['texto']))\n",
    "print('Noticia %d:' % k, '\\n', data['texto'][k])"
   ]
  },
  {
   "source": [
    "# Preprocesamiento de los datos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de stopwords\n",
    "sw = pd.read_csv('spanish.txt', header=None, names=['stopwords'])\n",
    "stopwords = sw['stopwords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar los datos.\n",
    "def preprocess(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub('[0-9]+', '', s) \n",
    "    s = re.sub('[!\"#$%&()*+,-./:;<=>¿?@[\\\\]^_`{|}~\\t—’‘“”]', '', s)\n",
    "    tokens = nltk.tokenize.word_tokenize(s) \n",
    "    tokens = [t for t in tokens if t not in stopwords] \n",
    "    tokens = [unidecode.unidecode(t) for t in tokens]\n",
    "    jtokens = ' '.join(tokens)\n",
    "    return jtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "pdata = [preprocess(t) for t in data['texto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Noticia %d :' % k, '\\n', pdata[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conteo de palabras por texto\n",
    "vect = CountVectorizer(max_features=3000)\n",
    "vdat = vect.fit_transform(pdata)\n",
    "data1 = pd.DataFrame(vdat.toarray(), columns=vect.get_feature_names())\n",
    "data1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conteo de palabras y ngrams por texto\n",
    "vect = CountVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "vdat = vect.fit_transform(pdata)\n",
    "data2 = pd.DataFrame(vdat.toarray(), columns=vect.get_feature_names())\n",
    "data2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frecuencia de palabras por texto\n",
    "data1sum = data1.sum(axis=1)\n",
    "data3 = data1.divide(data1sum, axis=0)\n",
    "data3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frecuencia de palabras y ngrams por texto\n",
    "data2sum = data2.sum(axis=1)\n",
    "data4 = data2.divide(data2sum, axis=0)\n",
    "data4.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf de palabras por texto\n",
    "vect = TfidfVectorizer(max_features=3000)\n",
    "vdat = vect.fit_transform(pdata)\n",
    "data5 = pd.DataFrame(vdat.toarray(), columns=vect.get_feature_names())\n",
    "data5.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf de palabras por texto\n",
    "vect = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "vdat = vect.fit_transform(pdata)\n",
    "data6 = pd.DataFrame(vdat.toarray(), columns=vect.get_feature_names())\n",
    "data6.head(1)"
   ]
  },
  {
   "source": [
    "# Separación de la muestra"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la variable y se toman muestras\n",
    "y = data['response']\n",
    "X = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.35, stratify=y, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.2, stratify=y_val, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = np.array([y_train.shape[0], y_val.shape[0], y_test.shape[0]])\n",
    "print(samples)\n",
    "print((samples/sum(samples)*100).round())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_train = y_train.index\n",
    "i_val = y_val.index\n",
    "i_test = y_test.index"
   ]
  },
  {
   "source": [
    "# Funciones y dataframes útiles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular las métricas de evaluación. \n",
    "def get_metrics(modelo, y, y_pred, y_pred_proba):\n",
    "    return pd.DataFrame({\n",
    "                'CV-Score': searcher.best_score_,\n",
    "                'Accuracy': accuracy_score(y, y_pred),\n",
    "                'AUC': roc_auc_score(y, y_pred_proba),\n",
    "                'F1 Score': f1_score(y, y_pred)},\n",
    "                index=[modelo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_X_train(X):\n",
    "    X_train = X.loc[i_train]\n",
    "    X_val = X.loc[i_val]\n",
    "    X_test = X.loc[i_test]\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(pred_val, pred_test, modelo, y_pred_proba_val, y_pred_proba_test):\n",
    "    pred_val[modelo] = y_pred_proba_val\n",
    "    pred_test[modelo] = y_pred_proba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames vacíos para almacenar los resultados \n",
    "results = pd.DataFrame()\n",
    "parametros = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario con los datos y sus nombres\n",
    "keys = ['data1', 'data2', 'data3', 'data4', 'data5', 'data6']\n",
    "values = [data1, data2, data3, data4, data5, data6]\n",
    "datos = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos primera ronda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=123)\n",
    "parameters = {'C':np.logspace(-4, 4, 20), \n",
    "               'penalty':['l1', 'l2']}\n",
    "searcher = GridSearchCV(estimator=model, \n",
    "                        param_grid=parameters, \n",
    "                        scoring='f1',\n",
    "                        n_jobs=-1, \n",
    "                        verbose=1)\n",
    "\n",
    "for key, value in datos.items():\n",
    "    X_train, X_val, X_test = gen_X_train(value)\n",
    "    \n",
    "    searcher.fit(X_train, y_train)\n",
    "    \n",
    "    print(key, \"Best CV params\", searcher.best_params_)\n",
    "    parametros = parametros.append(pd.DataFrame(searcher.best_params_, index=[key]))\n",
    "\n",
    "    best_model = searcher.best_estimator_\n",
    "    y_pred_train = best_model.predict(X_train)\n",
    "    y_pred_proba_train = best_model.predict_proba(X_train)[:,1]\n",
    "    results = results.append(get_metrics(key, y_train, y_pred_train, y_pred_proba_train))"
   ]
  },
  {
   "source": [
    "# Resultados primera ronda"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Best models:')\n",
    "print(results.idxmax(), '\\n')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros.to_csv('parametros_1.csv')\n",
    "parametros"
   ]
  },
  {
   "source": [
    "# Ajuste de los modelos a la muestra completa"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames vacíos para almacenar los resultados \n",
    "pred_val = pd.DataFrame()\n",
    "pred_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, value in datos.items():\n",
    "    X_train, X_val, X_test = gen_X_train(value)\n",
    "\n",
    "    model = LogisticRegression(random_state=123,\n",
    "                                C=parametros.loc[key][0],\n",
    "                                penalty=parametros.loc[key][1])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba_val = model.predict_proba(X_val)[:,1]\n",
    "    y_pred_proba_test = model.predict_proba(X_test)[:,1]\n",
    "    save_preds(pred_val, pred_test, key, y_pred_proba_val, y_pred_proba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val.index = i_val\n",
    "pred_test.index = i_test"
   ]
  },
  {
   "source": [
    "# Segunda ronda: Stacking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pred_val\n",
    "X_test = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames vacíos para almacenar los resultados \n",
    "parametros = pd.DataFrame()\n",
    "results = pd.DataFrame()\n",
    "pred_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelo = 'LR'\n",
    "model = LogisticRegression(random_state=123)\n",
    "parameters = {'C':np.logspace(-4, 4, 20), \n",
    "              'penalty':['l1', 'l2']}\n",
    "searcher = GridSearchCV(estimator=model, \n",
    "                        param_grid=parameters, \n",
    "                        scoring='f1',\n",
    "                        n_jobs=-1, \n",
    "                        verbose=1)\n",
    "searcher.fit(X_val, y_val)\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "parametros = parametros.append(pd.DataFrame(searcher.best_params_, index=[key]))\n",
    "\n",
    "best_model = searcher.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "pred_test[modelo] = y_pred_proba\n",
    "\n",
    "results = results.append(get_metrics(modelo, y_test, y_pred, y_pred_proba))\n",
    "results.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros.to_csv('parametros_2.csv')\n",
    "parametros"
   ]
  },
  {
   "source": [
    "# Ajuste del mejor modelo a la muestra completa"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = 'LR'\n",
    "model = LogisticRegression(random_state=123,\n",
    "                            C=parametros['C'].iloc[0],\n",
    "                            penalty=parametros['penalty'].iloc[0])\n",
    "model.fit(X_val, y_val)\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "source": [
    "# Optimizar threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar probabilidad en label\n",
    "def to_labels(y_pred_proba, threshold):\n",
    "\treturn (y_pred_proba >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferentes thresholds a testear\n",
    "thresholds = np.linspace(0, 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimización del threshold en base a F1 Score\n",
    "scores = [f1_score(y_test, to_labels(y_pred_proba, t)) for t in thresholds]\n",
    "ix = np.argmax(scores)\n",
    "print(modelo, 'Threshold=%.4f, F-Score=%.4f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred_proba >= thresholds[ix]).astype(int)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['y_pred'], index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Accuracy=%.4f' % accuracy_score(y_test, y_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.concat([y_test, y_pred], axis=1)\n",
    "pred['Accuracy'] = (pred['response']==pred['y_pred'])\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}